{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Word Prediction by Edulakanti Richa Reddy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=requests.get(\"http://www.gutenberg.org/cache/epub/5200/pg5200.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Metamorphosis, by Franz Kafka\\r\\nTranslated by David Wyllie.\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org\\r\\n\\r\\n** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\\r\\n**     Please follow the copyright guidelines in this file.     **\\r\\n\\r\\n\\r\\nTitle: Metamorphosis\\r\\n\\r\\nAuthor: Franz Kafka\\r\\n\\r\\nTranslator: David Wyllie\\r\\n\\r\\nRelease Date: August 16, 2005 [EBook #5200]\\r\\nFirst posted: May 13, 2002\\r\\nLast updated: May 20, 2012\\r\\n\\r\\nLanguage: English\\r\\n\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCopyright (C) 2002 David Wyllie.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n  Metamorphosis\\r\\n  Franz Kafka\\r\\n\\r\\nTranslated by David Wyllie\\r\\n\\r\\n\\r\\n\\r\\nI\\r\\n\\r\\n\\r\\nOne morning, when Gregor Samsa woke from troubled dreams, he found\\r\\nhimself transformed in his bed into a horrible vermin.  He lay on\\r\\nhis armour-like back, and if he lifted his head a little he could\\r\\nsee his brown belly, slightly domed and divided by arches into stiff\\r\\nsections.  The bedding was hardly able to cover it and seemed ready\\r\\nto slide off any moment.  His many legs, pitifully thin compared\\r\\nwith the size of the rest of him, waved about helplessly as he\\r\\nlooked.\\r\\n\\r\\n\"What\\'s happened to me?\" he thought.  It wasn\\'t a dream.  His room,\\r\\na proper human room although a little too small, lay peacefully\\r\\nbetwee'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[:1500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Metamorphosis, by Franz Kafka\\r'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = response.text.split('\\n')\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'away from the bed, bend down with the load and then be patient and\\r'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[253:]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2110"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we have a list of the lines in the data. Now we are going to join all the lines and create a long string consisting of the data in continuous format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'away from the bed, bend down with the load and then be patient and\\r careful as he swang over onto the floor, where, hopefully, the\\r little legs would find a use.  Should he really call for help\\r though, even apart from the fact that all the doors were locked?\\r Despite all the difficulty he was in, he could not suppress a smile\\r at this thought.\\r \\r After a while he had already moved so far across that it would have\\r been hard for him to keep his balance if he rocked too hard.  The\\r time was now ten past seven and he would have to make a final\\r decision very soon.  Then there was a ring at the door of the flat.\\r \"That\\'ll be someone from work\", he said to himself, and froze very\\r still, although his little legs only became all the more lively as\\r they danced around.  For a moment everything remained quiet.\\r \"They\\'re not opening the door\", Gregor said to himself, caught in\\r some nonsensical hope.  But then of course, the maid\\'s firm steps\\r went to the door as ever and opened it.  Gregor on'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \" \".join(data)\n",
    "data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['away', 'from', 'the', 'bed', 'bend', 'down', 'with', 'the', 'load', 'and', 'then', 'be', 'patient', 'and', 'careful', 'as', 'he', 'swang', 'over', 'onto', 'the', 'floor', 'where', 'hopefully', 'the', 'little', 'legs', 'would', 'find', 'a', 'use', 'should', 'he', 'really', 'call', 'for', 'help', 'though', 'even', 'apart', 'from', 'the', 'fact', 'that', 'all', 'the', 'doors', 'were', 'locked', 'despite']\n"
     ]
    }
   ],
   "source": [
    "def clean_text(doc):\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "tokens = clean_text(data)\n",
    "print(tokens[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22607"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to use a set of previous words to predict the next word in the sentence. To be precise we are going to use a set of 50 words to predict the 51st word. Hence we are going to divide our\n",
    "data in chunks of 51 words and at the last we will separate the last word from every line. We are going to limit our dataset to 200000 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22556\n"
     ]
    }
   ],
   "source": [
    "length = 50 + 1\n",
    "lines = []\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    lines.append(line)\n",
    "    if i > 200000:\n",
    "        break\n",
    "print(len(lines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LSTM Model and Prepare X and Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import all the necessary libraries used to pre-process the data and create the layers of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a unique numerical token for each unique word in the dataset.fit_on_texts() updates internal vocabulary based on a list of texts. texts_to_sequences() transforms each text in\n",
    "texts to a sequence of integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequences containes a list of integer values created by tokenizer. Each line in sequences has 51 words. Now we will split each line such that the first 50 words are in X and the last word is in y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 103,   29,    1,  245, 2883,   98,   14,    1, 1435,    3,   48,\n",
       "         30,  618,    3,  756,   13,    6, 1434,  107,  165,    1,  149,\n",
       "         86, 2880,    1,   78,  225,   21,  530,   12,  156,  193,    6,\n",
       "        142,  754,   17,  180,  116,   49, 1433,   29,    1,  753,   11,\n",
       "         26,    1,  455,   58,  617,  329])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:,-1]\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab_size contains all the uniques words in the dataset. tokenizer.word_index gives the mapping of each unique word to its numerical equivalent. Hence len() of tokenizer.word_index gives the\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to_categorical() converts a class vector (integers) to binary class matrix. num_classes is the total number of classes which is vocab_size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = X.shape[1]\n",
    "seq_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            144250    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2885)              291385    \n",
      "=================================================================\n",
      "Total params: 586,535\n",
      "Trainable params: 586,535\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compiling the model we will now train the model using model.fit() on the training dataset. We will use 100 epochs to train the model. An epoch is an iteration over the entire x and y data\n",
    "provided. batch_size is the number of samples per gradient update i.e. the weights will be updates after 256 training examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "89/89 [==============================] - 19s 175ms/step - loss: 6.6514 - accuracy: 0.0442\n",
      "Epoch 2/100\n",
      "89/89 [==============================] - 16s 177ms/step - loss: 6.1883 - accuracy: 0.0540s - loss: 6.1891 - accura\n",
      "Epoch 3/100\n",
      "89/89 [==============================] - 15s 174ms/step - loss: 6.1568 - accuracy: 0.0540\n",
      "Epoch 4/100\n",
      "89/89 [==============================] - 16s 176ms/step - loss: 6.0366 - accuracy: 0.0540\n",
      "Epoch 5/100\n",
      "89/89 [==============================] - 16s 183ms/step - loss: 5.9040 - accuracy: 0.0571\n",
      "Epoch 6/100\n",
      "89/89 [==============================] - 15s 173ms/step - loss: 5.7599 - accuracy: 0.0627\n",
      "Epoch 7/100\n",
      "89/89 [==============================] - 15s 174ms/step - loss: 5.6735 - accuracy: 0.0683\n",
      "Epoch 8/100\n",
      "89/89 [==============================] - 15s 174ms/step - loss: 5.6093 - accuracy: 0.0714\n",
      "Epoch 9/100\n",
      "89/89 [==============================] - 16s 175ms/step - loss: 5.5571 - accuracy: 0.0781\n",
      "Epoch 10/100\n",
      "89/89 [==============================] - 16s 176ms/step - loss: 5.5040 - accuracy: 0.0816\n",
      "Epoch 11/100\n",
      "89/89 [==============================] - 16s 176ms/step - loss: 5.4516 - accuracy: 0.0884\n",
      "Epoch 12/100\n",
      "89/89 [==============================] - 16s 174ms/step - loss: 5.3937 - accuracy: 0.0904\n",
      "Epoch 13/100\n",
      "89/89 [==============================] - 16s 176ms/step - loss: 5.3269 - accuracy: 0.0956\n",
      "Epoch 14/100\n",
      "89/89 [==============================] - 16s 178ms/step - loss: 5.2639 - accuracy: 0.1027\n",
      "Epoch 15/100\n",
      "89/89 [==============================] - 17s 187ms/step - loss: 5.1932 - accuracy: 0.1078\n",
      "Epoch 16/100\n",
      "89/89 [==============================] - 16s 175ms/step - loss: 5.1232 - accuracy: 0.1142\n",
      "Epoch 17/100\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 5.0547 - accuracy: 0.1181\n",
      "Epoch 18/100\n",
      "89/89 [==============================] - 19s 216ms/step - loss: 4.9961 - accuracy: 0.1233\n",
      "Epoch 19/100\n",
      "89/89 [==============================] - 19s 208ms/step - loss: 4.9448 - accuracy: 0.1256\n",
      "Epoch 20/100\n",
      "89/89 [==============================] - 19s 219ms/step - loss: 4.8950 - accuracy: 0.1295\n",
      "Epoch 21/100\n",
      "89/89 [==============================] - 18s 200ms/step - loss: 4.8543 - accuracy: 0.1318\n",
      "Epoch 22/100\n",
      "89/89 [==============================] - 17s 187ms/step - loss: 4.8067 - accuracy: 0.1364\n",
      "Epoch 23/100\n",
      "89/89 [==============================] - 17s 192ms/step - loss: 4.7720 - accuracy: 0.1396\n",
      "Epoch 24/100\n",
      "89/89 [==============================] - 22s 246ms/step - loss: 4.7272 - accuracy: 0.1395\n",
      "Epoch 25/100\n",
      "89/89 [==============================] - 22s 242ms/step - loss: 4.6860 - accuracy: 0.1434\n",
      "Epoch 26/100\n",
      "89/89 [==============================] - 22s 247ms/step - loss: 4.6457 - accuracy: 0.1466\n",
      "Epoch 27/100\n",
      "89/89 [==============================] - 18s 198ms/step - loss: 4.6065 - accuracy: 0.1501\n",
      "Epoch 28/100\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 4.5677 - accuracy: 0.1512\n",
      "Epoch 29/100\n",
      "89/89 [==============================] - 18s 204ms/step - loss: 4.5279 - accuracy: 0.1533\n",
      "Epoch 30/100\n",
      "89/89 [==============================] - 18s 201ms/step - loss: 4.4927 - accuracy: 0.1565\n",
      "Epoch 31/100\n",
      "89/89 [==============================] - 17s 193ms/step - loss: 4.4606 - accuracy: 0.1565\n",
      "Epoch 32/100\n",
      "89/89 [==============================] - 18s 199ms/step - loss: 4.4288 - accuracy: 0.1582\n",
      "Epoch 33/100\n",
      "89/89 [==============================] - 19s 212ms/step - loss: 4.3936 - accuracy: 0.1604\n",
      "Epoch 34/100\n",
      "89/89 [==============================] - 16s 180ms/step - loss: 4.3638 - accuracy: 0.1605\n",
      "Epoch 35/100\n",
      "89/89 [==============================] - 17s 195ms/step - loss: 4.3358 - accuracy: 0.1613\n",
      "Epoch 36/100\n",
      "89/89 [==============================] - 17s 196ms/step - loss: 4.3070 - accuracy: 0.1630\n",
      "Epoch 37/100\n",
      "89/89 [==============================] - 19s 210ms/step - loss: 4.2755 - accuracy: 0.1642\n",
      "Epoch 38/100\n",
      "89/89 [==============================] - 18s 207ms/step - loss: 4.2450 - accuracy: 0.1660\n",
      "Epoch 39/100\n",
      "89/89 [==============================] - 18s 203ms/step - loss: 4.2147 - accuracy: 0.1690\n",
      "Epoch 40/100\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 4.1848 - accuracy: 0.1715\n",
      "Epoch 41/100\n",
      "89/89 [==============================] - 17s 187ms/step - loss: 4.1590 - accuracy: 0.1714\n",
      "Epoch 42/100\n",
      "89/89 [==============================] - 17s 195ms/step - loss: 4.1304 - accuracy: 0.1716\n",
      "Epoch 43/100\n",
      "89/89 [==============================] - 19s 213ms/step - loss: 4.1044 - accuracy: 0.1745\n",
      "Epoch 44/100\n",
      "89/89 [==============================] - 19s 217ms/step - loss: 4.0777 - accuracy: 0.1761\n",
      "Epoch 45/100\n",
      "89/89 [==============================] - 19s 216ms/step - loss: 4.0539 - accuracy: 0.1780\n",
      "Epoch 46/100\n",
      "89/89 [==============================] - 20s 223ms/step - loss: 4.0218 - accuracy: 0.1794\n",
      "Epoch 47/100\n",
      "89/89 [==============================] - 19s 217ms/step - loss: 3.9964 - accuracy: 0.1830\n",
      "Epoch 48/100\n",
      "89/89 [==============================] - 21s 231ms/step - loss: 3.9706 - accuracy: 0.1853\n",
      "Epoch 49/100\n",
      "89/89 [==============================] - 19s 214ms/step - loss: 3.9452 - accuracy: 0.1854\n",
      "Epoch 50/100\n",
      "89/89 [==============================] - 19s 214ms/step - loss: 3.9179 - accuracy: 0.1890\n",
      "Epoch 51/100\n",
      "89/89 [==============================] - 20s 221ms/step - loss: 3.8922 - accuracy: 0.1912\n",
      "Epoch 52/100\n",
      "89/89 [==============================] - 20s 221ms/step - loss: 3.8660 - accuracy: 0.1926\n",
      "Epoch 53/100\n",
      "89/89 [==============================] - 19s 208ms/step - loss: 3.8371 - accuracy: 0.1962\n",
      "Epoch 54/100\n",
      "89/89 [==============================] - 21s 232ms/step - loss: 3.8117 - accuracy: 0.2003\n",
      "Epoch 55/100\n",
      "89/89 [==============================] - 21s 236ms/step - loss: 3.7863 - accuracy: 0.1995\n",
      "Epoch 56/100\n",
      "89/89 [==============================] - 22s 247ms/step - loss: 3.7551 - accuracy: 0.2046\n",
      "Epoch 57/100\n",
      "89/89 [==============================] - 21s 232ms/step - loss: 3.7348 - accuracy: 0.2068\n",
      "Epoch 58/100\n",
      "89/89 [==============================] - 21s 235ms/step - loss: 3.7064 - accuracy: 0.2111\n",
      "Epoch 59/100\n",
      "89/89 [==============================] - 20s 219ms/step - loss: 3.6762 - accuracy: 0.2134\n",
      "Epoch 60/100\n",
      "89/89 [==============================] - 20s 228ms/step - loss: 3.6472 - accuracy: 0.2153\n",
      "Epoch 61/100\n",
      "89/89 [==============================] - 20s 223ms/step - loss: 3.6297 - accuracy: 0.2181\n",
      "Epoch 62/100\n",
      "89/89 [==============================] - 18s 202ms/step - loss: 3.6007 - accuracy: 0.2217\n",
      "Epoch 63/100\n",
      "89/89 [==============================] - 20s 225ms/step - loss: 3.5735 - accuracy: 0.2218\n",
      "Epoch 64/100\n",
      "89/89 [==============================] - 19s 210ms/step - loss: 3.5477 - accuracy: 0.2280\n",
      "Epoch 65/100\n",
      "89/89 [==============================] - 18s 204ms/step - loss: 3.5259 - accuracy: 0.2296\n",
      "Epoch 66/100\n",
      "89/89 [==============================] - 19s 209ms/step - loss: 3.4958 - accuracy: 0.2334\n",
      "Epoch 67/100\n",
      "89/89 [==============================] - 18s 204ms/step - loss: 3.4777 - accuracy: 0.2371\n",
      "Epoch 68/100\n",
      "89/89 [==============================] - 19s 213ms/step - loss: 3.4516 - accuracy: 0.2412\n",
      "Epoch 69/100\n",
      "89/89 [==============================] - 20s 225ms/step - loss: 3.4306 - accuracy: 0.2423\n",
      "Epoch 70/100\n",
      "89/89 [==============================] - 19s 214ms/step - loss: 3.4001 - accuracy: 0.2478\n",
      "Epoch 71/100\n",
      "89/89 [==============================] - 19s 208ms/step - loss: 3.3736 - accuracy: 0.2523\n",
      "Epoch 72/100\n",
      "89/89 [==============================] - 19s 213ms/step - loss: 3.3525 - accuracy: 0.2536\n",
      "Epoch 73/100\n",
      "89/89 [==============================] - 20s 220ms/step - loss: 3.3314 - accuracy: 0.2572\n",
      "Epoch 74/100\n",
      "89/89 [==============================] - 19s 211ms/step - loss: 3.3114 - accuracy: 0.2600\n",
      "Epoch 75/100\n",
      "89/89 [==============================] - 18s 206ms/step - loss: 3.2904 - accuracy: 0.2645\n",
      "Epoch 76/100\n",
      "89/89 [==============================] - 20s 222ms/step - loss: 3.2672 - accuracy: 0.2656\n",
      "Epoch 77/100\n",
      "89/89 [==============================] - 19s 215ms/step - loss: 3.2459 - accuracy: 0.2708\n",
      "Epoch 78/100\n",
      "89/89 [==============================] - 20s 220ms/step - loss: 3.2244 - accuracy: 0.2737\n",
      "Epoch 79/100\n",
      "89/89 [==============================] - 20s 227ms/step - loss: 3.2091 - accuracy: 0.2754\n",
      "Epoch 80/100\n",
      "89/89 [==============================] - 20s 222ms/step - loss: 3.1848 - accuracy: 0.2816\n",
      "Epoch 81/100\n",
      "89/89 [==============================] - 20s 219ms/step - loss: 3.1607 - accuracy: 0.2856\n",
      "Epoch 82/100\n",
      "89/89 [==============================] - 19s 215ms/step - loss: 3.1418 - accuracy: 0.2864\n",
      "Epoch 83/100\n",
      "89/89 [==============================] - 20s 223ms/step - loss: 3.1209 - accuracy: 0.2887\n",
      "Epoch 84/100\n",
      "89/89 [==============================] - 18s 200ms/step - loss: 3.1025 - accuracy: 0.2968\n",
      "Epoch 85/100\n",
      "89/89 [==============================] - 19s 214ms/step - loss: 3.0835 - accuracy: 0.2979\n",
      "Epoch 86/100\n",
      "89/89 [==============================] - 19s 214ms/step - loss: 3.0668 - accuracy: 0.2987\n",
      "Epoch 87/100\n",
      "89/89 [==============================] - 20s 228ms/step - loss: 3.0514 - accuracy: 0.3046\n",
      "Epoch 88/100\n",
      "89/89 [==============================] - 19s 216ms/step - loss: 3.0241 - accuracy: 0.3076\n",
      "Epoch 89/100\n",
      "89/89 [==============================] - 20s 224ms/step - loss: 3.0131 - accuracy: 0.3094\n",
      "Epoch 90/100\n",
      "89/89 [==============================] - 20s 227ms/step - loss: 2.9883 - accuracy: 0.3117\n",
      "Epoch 91/100\n",
      "89/89 [==============================] - 20s 220ms/step - loss: 2.9684 - accuracy: 0.3143\n",
      "Epoch 92/100\n",
      "89/89 [==============================] - 20s 222ms/step - loss: 2.9528 - accuracy: 0.3181\n",
      "Epoch 93/100\n",
      "89/89 [==============================] - 19s 210ms/step - loss: 2.9355 - accuracy: 0.3222\n",
      "Epoch 94/100\n",
      "89/89 [==============================] - 19s 214ms/step - loss: 2.9167 - accuracy: 0.3269\n",
      "Epoch 95/100\n",
      "89/89 [==============================] - 17s 189ms/step - loss: 2.8912 - accuracy: 0.3309\n",
      "Epoch 96/100\n",
      "89/89 [==============================] - 17s 191ms/step - loss: 2.8743 - accuracy: 0.3335\n",
      "Epoch 97/100\n",
      "89/89 [==============================] - 18s 199ms/step - loss: 2.8604 - accuracy: 0.3357\n",
      "Epoch 98/100\n",
      "89/89 [==============================] - 17s 191ms/step - loss: 2.8428 - accuracy: 0.3386\n",
      "Epoch 99/100\n",
      "89/89 [==============================] - 17s 190ms/step - loss: 2.8243 - accuracy: 0.3405\n",
      "Epoch 100/100\n",
      "89/89 [==============================] - 17s 192ms/step - loss: 2.8117 - accuracy: 0.3462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cb0445be80>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size = 256, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to generate words using the model. For this we need a set of 50 words to predict the 51st word. So we are taking a random line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'condition seemed serious enough to remind even his father that gregor despite his current sad and revolting form was a family member who could not be treated as an enemy on the contrary as a family there was a duty to swallow any revulsion for him and to be patient just'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text=lines[12343]\n",
    "seed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate_text_seq() generates n_words number of words after the given seed_text. We are going to pre-process the seed_text before predicting. We are going to encode the seed_text using the\n",
    "same encoding used for encoding the training data. Then we are going to convert the seed_textto 50 words by using pad_sequences(). Now we will predict using model.predict_classes(). After that\n",
    "we will search the word in tokenizer using the index in y_predict. Finally we will append the predicted word to seed_text and text and repeat the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\n",
    "    text = []\n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')\n",
    "        y_predict = model.predict_classes(encoded)\n",
    "        predicted_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == y_predict:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        seed_text = seed_text + ' ' + predicted_word\n",
    "        text.append(predicted_word)\n",
    "    return ' '.join(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the next 100 words are predicted by the model for the seed_text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to do the time of year to crawl about mindlessly as he had been dozing and he could have checked for herself and stamped the little contracts on the floor he had to do the time of the living room free often stubborn and ave him to do he was not hungry there was not been sold he had to do the time of the living room and made her to the conservatory that he was not been sold he had to do the time of her hand he had to do the time of her hand he had to'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_seq(model, tokenizer, seq_length, seed_text, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got a accuracy of 46%. To increase the accuracy we can increase the number of epochs or we can consider the entire data for training. For this model we have only considered 1/4th of the\n",
    "data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
